# To download a single file
wget http://path.to.the/file

# To download a file and change its name
wget http://path.to.the/file -O newname

# To download a file into a directory
wget -P path/to/directory http://path.to.the/file

# To continue an aborted downloaded
wget -c http://path.to.the/file

# To download multiples files with multiple URLs
wget URL1 URL2

# To parse a file that contains a list of URLs to fetch each one
wget -i url_list.txt

# To mirror a whole page locally
wget -pk http://path.to.the/page.html

# To mirror a whole site locally
wget -mk http://site.tl/

# To download files according to a pattern
wget http://www.myserver.com/files-{1..15}.tar.bz2

# To download all the files in a directory with a specific extension if directory indexing is enabled
wget -r -l1 -A.extension http://myserver.com/directory

# Allows you to download just the headers of responses (-S --spider) and display them on Stdout (-O -).
wget -S --spider -O - http://google.com

# Change the User-Agent to 'User-Agent: toto'
wget -U 'toto' http://google.com

# Scrape: download single but complete pages (articles etc…)
wget -e robots=off --page-requisites --adjust-extension --convert-links

# Scrape: download complete sites
wget -e robots=off --mirror –w 2 --page-requisites --adjust-extension –-convert-links –P /home/user/sitecopy/

# How to download HTTP directory with all files and sub-directories as they appear on the online files/folders list?
wget -r -np -nH --cut-dirs=3 -R index.html http://hostname/aaa/bbb/ccc/ddd/
